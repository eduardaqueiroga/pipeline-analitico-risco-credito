{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %pip install -q ydata-profiling xgboost\n",
        "# %pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# ------------------------- Imports ---------------------------\n",
        "import os, sys, gc, json, zipfile, math, warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, roc_curve, average_precision_score,\n",
        "    precision_recall_fscore_support, accuracy_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "pd.set_option('display.max_columns', 120)\n",
        "\n",
        "print('Libs carregadas. pandas =', pd.__version__)\n",
        "\n",
        "# ------------------- Parâmetros/Flags -----------------------\n",
        "# Você baixará a base manualmente. Escolha UMA das opções de entrada abaixo:\n",
        "INPUT_MODE = 'UPLOAD'   # 'UPLOAD' ou 'DRIVE'\n",
        "\n",
        "GENERATE_PROFILE = False   # True: gera relatório HTML com ydata-profiling (pesado)\n",
        "SAMPLE_ROWS = 200_000      # Amostragem para performance (None para usar tudo)\n",
        "\n",
        "DATA_PATH = Path('data')\n",
        "DATA_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "# ----------------- Funções utilitárias ----------------------\n",
        "def build_data_dictionary(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Dicionário de dados básico para governança/qualidade.\"\"\"\n",
        "    dd = []\n",
        "    for c in df.columns:\n",
        "        n_null = int(df[c].isna().sum())\n",
        "        pct_null = float(n_null/len(df)) * 100 if len(df) else 0.0\n",
        "        n_unique = int(df[c].nunique(dropna=True))\n",
        "        example = df[c].dropna().iloc[0] if df[c].dropna().shape[0] else None\n",
        "        dd.append({\n",
        "            'column': c,\n",
        "            'dtype': str(df[c].dtype),\n",
        "            'n_missing': n_null,\n",
        "            'pct_missing': round(pct_null, 2),\n",
        "            'n_unique': n_unique,\n",
        "            'example': example\n",
        "        })\n",
        "    return pd.DataFrame(dd).sort_values(['pct_missing','column'], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "def _read_lending_club_csv(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Lê CSV do Lending Club:\n",
        "    - Muitos LoanStats têm 1ª linha de metadados -> skiprows=1\n",
        "    - Remove linhas completamente vazias no final\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, low_memory=False)\n",
        "    df = df.dropna(how='all')\n",
        "    if 'loan_status' not in df.columns:\n",
        "        raise ValueError('Coluna loan_status não encontrada. Use LoanStats_*.csv válido.')\n",
        "    return df\n",
        "\n",
        "def load_lending_club(raw_file: Path) -> pd.DataFrame:\n",
        "    \"\"\"Lê CSV ou ZIP contendo CSV do Lending Club.\"\"\"\n",
        "    if raw_file is None or not Path(raw_file).exists():\n",
        "        raise FileNotFoundError('Arquivo não encontrado. Faça upload ou aponte o caminho no DRIVE_FILE.')\n",
        "    raw_file = Path(raw_file)\n",
        "    if raw_file.suffix == '.zip':\n",
        "        with zipfile.ZipFile(raw_file, 'r') as z:\n",
        "            csv_names = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
        "            if not csv_names:\n",
        "                raise ValueError('ZIP não contém CSVs. Verifique o conteúdo.')\n",
        "            # Se houver mais de um csv, pegue o maior (geralmente a base principal)\n",
        "            csv_names_sorted = sorted(csv_names, key=lambda n: z.getinfo(n).file_size, reverse=True)\n",
        "            target_csv = csv_names_sorted[0]\n",
        "            print('Lendo CSV dentro do zip:', target_csv)\n",
        "            with z.open(target_csv) as f:\n",
        "                df = pd.read_csv(f, low_memory=False)\n",
        "                df = df.dropna(how='all')\n",
        "    else:\n",
        "        df = _read_lending_club_csv(raw_file)\n",
        "    return df\n",
        "\n",
        "def ks_statistic(y_true, y_prob):\n",
        "    \"\"\"KS e threshold correspondente (máx TPR-FPR).\"\"\"\n",
        "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
        "    ks = np.max(tpr - fpr)\n",
        "    ix = np.argmax(tpr - fpr)\n",
        "    return float(ks), float(thr[ix])\n",
        "\n",
        "def top_decile_lift(y_true, y_prob):\n",
        "    \"\"\"Lift e capture rate no top decil de score.\"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    cutoff = np.quantile(y_prob, 0.9)\n",
        "    top_mask = y_prob >= cutoff\n",
        "    rate_overall = y_true.mean() if len(y_true) else 0.0\n",
        "    rate_top = y_true[top_mask].mean() if top_mask.any() else 0.0\n",
        "    lift = (rate_top / rate_overall) if rate_overall > 0 else np.nan\n",
        "    capture = y_true[top_mask].sum() / y_true.sum() if y_true.sum() > 0 else np.nan\n",
        "    return float(lift), float(capture)\n",
        "\n",
        "# --------------- Entrada de dados: Upload ou Drive ----------\n",
        "RAW_FILE = None\n",
        "\n",
        "if INPUT_MODE.upper() == 'UPLOAD':\n",
        "    print('>> Modo UPLOAD: usando arquivo LoanStats.csv existente')\n",
        "    src = Path('/content/LoanStats.csv')  # Path to the existing file\n",
        "    if not src.exists():\n",
        "        raise FileNotFoundError(f'Não encontrei o arquivo: {src}. Faça upload ou aponte o caminho no DRIVE_FILE.')\n",
        "    dst = DATA_PATH / src.name\n",
        "    import shutil; shutil.copy(str(src), str(dst))\n",
        "    RAW_FILE = dst\n",
        "    print('Arquivo copiado para', RAW_FILE)\n",
        "\n",
        "elif INPUT_MODE.upper() == 'DRIVE':\n",
        "    print('>> Modo DRIVE: montando o Google Drive...')\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        RAW_FILE = Path(DRIVE_FILE)\n",
        "        if not RAW_FILE.exists():\n",
        "            raise FileNotFoundError(f'Não encontrei o arquivo no Drive: {RAW_FILE}')\n",
        "        # Copia para data/ para padronizar\n",
        "        dst = DATA_PATH / RAW_FILE.name\n",
        "        import shutil; shutil.copy(str(RAW_FILE), str(dst))\n",
        "        RAW_FILE = dst\n",
        "        print('Arquivo copiado para', RAW_FILE)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError('Falha ao acessar o Drive. Erro: ' + str(e))\n",
        "else:\n",
        "    raise ValueError(\"INPUT_MODE inválido. Use 'UPLOAD' ou 'DRIVE'.\")\n",
        "\n",
        "print('Arquivo detectado:', RAW_FILE)\n",
        "\n",
        "# ------------------ Leitura e dicionário --------------------\n",
        "df_raw = load_lending_club(RAW_FILE)\n",
        "print('Shape bruto:', df_raw.shape)\n",
        "\n",
        "data_dict_raw = build_data_dictionary(df_raw)\n",
        "data_dict_raw.to_csv(DATA_PATH/'data_dictionary.csv', index=False)\n",
        "print('Dicionário (raw) salvo em', DATA_PATH/'data_dictionary.csv')\n",
        "\n",
        "# ----------- (Opcional) Perfil de qualidade (HTML) ----------\n",
        "if GENERATE_PROFILE:\n",
        "    try:\n",
        "        from ydata_profiling import ProfileReport\n",
        "        profile = ProfileReport(df_raw, title='Data Quality – Lending Club', explorative=True)\n",
        "        profile_path = DATA_PATH/'data_quality_profile.html'\n",
        "        profile.to_file(profile_path)\n",
        "        print('Relatório de qualidade salvo em', profile_path)\n",
        "    except Exception as e:\n",
        "        print('Falha ao gerar perfil de qualidade (ydata-profiling):', e)\n",
        "else:\n",
        "    print('Pulando geração do perfil para rodar mais leve.')\n",
        "\n",
        "# --------- Preprocessamento e feature engineering -----------\n",
        "DEFAULT_STATUSES = {\n",
        "    'Charged Off','Default','Late (31-120 days)','Late (16-30 days)',\n",
        "    'Does not meet the credit policy. Status:Charged Off'\n",
        "}\n",
        "NONDEFAULT_STATUSES = {\n",
        "    'Fully Paid', 'Does not meet the credit policy. Status:Fully Paid'\n",
        "}\n",
        "\n",
        "def preprocess(df: pd.DataFrame, sample_rows: int = SAMPLE_ROWS):\n",
        "    \"\"\"\n",
        "    - Filtra statuses bem definidos e cria alvo binário 'default'.\n",
        "    - Converte percentuais: int_rate, revol_util.\n",
        "    - Converte datas: issue_d, earliest_cr_line; cria 'credit_history_months'.\n",
        "    - Extrai 'term_months' a partir de 'term'.\n",
        "    - Cria 'log_annual_inc' (transformação log).\n",
        "    - Seleciona features comuns ao crédito.\n",
        "    - Amostra linhas para performance, se configurado.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = df[df['loan_status'].isin(DEFAULT_STATUSES.union(NONDEFAULT_STATUSES))].copy()\n",
        "    df['default'] = df['loan_status'].apply(lambda s: 1 if s in DEFAULT_STATUSES else 0)\n",
        "\n",
        "    for col in ['int_rate','revol_util']:\n",
        "        if col in df.columns:\n",
        "            df[col] = (df[col].astype(str).str.replace('%','', regex=False)\n",
        "                                 .replace('nan', np.nan).astype(float))\n",
        "\n",
        "    for col in ['issue_d','earliest_cr_line']:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], format='%b-%Y', errors='coerce')\n",
        "\n",
        "    if 'issue_d' in df.columns and 'earliest_cr_line' in df.columns:\n",
        "        chm = (\n",
        "            (df['issue_d'].dt.year - df['earliest_cr_line'].dt.year) * 12\n",
        "            + (df['issue_d'].dt.month - df['earliest_cr_line'].dt.month)\n",
        "        )\n",
        "        df['credit_history_months'] = chm.astype('float')\n",
        "\n",
        "    if 'term' in df.columns:\n",
        "        df['term_months'] = df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
        "\n",
        "    if 'annual_inc' in df.columns:\n",
        "        df['log_annual_inc'] = np.log1p(pd.to_numeric(df['annual_inc'], errors='coerce'))\n",
        "\n",
        "    candidate_features = [\n",
        "        'loan_amnt','funded_amnt','int_rate','installment','grade','sub_grade',\n",
        "        'emp_length','home_ownership','annual_inc','verification_status','purpose',\n",
        "        'dti','delinq_2yrs','inq_last_6mths','open_acc','pub_rec','revol_bal','revol_util',\n",
        "        'total_acc','issue_d','earliest_cr_line','term','credit_history_months','term_months',\n",
        "        'log_annual_inc'\n",
        "    ]\n",
        "    features = [c for c in candidate_features if c in df.columns]\n",
        "    base_cols = features + ['default']\n",
        "    df = df[base_cols].copy()\n",
        "\n",
        "    all_null_cols = [c for c in df.columns if df[c].isna().all()]\n",
        "    if all_null_cols:\n",
        "        df = df.drop(columns=all_null_cols)\n",
        "\n",
        "    if sample_rows and len(df) > sample_rows:\n",
        "        df = df.sample(n=sample_rows, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess(df_raw)\n",
        "print('Shape após preprocessamento:', df.shape)\n",
        "\n",
        "# --------------- Split, pipeline e treino -------------------\n",
        "target = 'default'\n",
        "feature_cols = [c for c in df.columns if c != target]\n",
        "num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
        "cat_cols = [c for c in feature_cols if c not in num_cols]\n",
        "print('Numéricas:', len(num_cols), '| Categóricas:', len(cat_cols))\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "y = df[target].astype(int).copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),('scaler', StandardScaler(with_mean=False))])\n",
        "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=True))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_cols),\n",
        "        ('cat', categorical_transformer, cat_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Modelo baseline: Logistic Regression\n",
        "log_reg = Pipeline(steps=[\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', LogisticRegression(max_iter=200, solver='saga'))\n",
        "])\n",
        "log_reg.fit(X_train, y_train)\n",
        "proba_lr = log_reg.predict_proba(X_test)[:, 1]\n",
        "auc_lr = roc_auc_score(y_test, proba_lr)\n",
        "print(f'AUC Logistic Regression: {auc_lr:.4f}')\n",
        "\n",
        "# Modelo opcional: XGBoost (se instalado)\n",
        "proba_xgb, auc_xgb, xgb = None, None, None\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb = Pipeline(steps=[\n",
        "        ('prep', preprocessor),\n",
        "        ('clf', XGBClassifier(\n",
        "            n_estimators=300,\n",
        "            max_depth=5,\n",
        "            learning_rate=0.08,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            eval_metric='auc',\n",
        "            tree_method='hist'\n",
        "        ))\n",
        "    ])\n",
        "    xgb.fit(X_train, y_train)\n",
        "    proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
        "    auc_xgb = roc_auc_score(y_test, proba_xgb)\n",
        "    print(f'AUC XGBoost: {auc_xgb:.4f}')\n",
        "except Exception as e:\n",
        "    print('XGBoost indisponível/erro. Usando apenas Logistic Regression. Detalhe:', e)\n",
        "\n",
        "# Seleciona o melhor por AUC\n",
        "if proba_xgb is not None and auc_xgb >= auc_lr:\n",
        "    best_model_name, best_model, y_score, auc_best = 'XGBoost', xgb, proba_xgb, auc_xgb\n",
        "else:\n",
        "    best_model_name, best_model, y_score, auc_best = 'LogisticRegression', log_reg, proba_lr, auc_lr\n",
        "\n",
        "print('Melhor modelo:', best_model_name, '| AUC:', round(float(auc_best), 4))\n",
        "\n",
        "# ---------------------- Avaliação ---------------------------\n",
        "auc = float(roc_auc_score(y_test, y_score))\n",
        "gini = float(2 * auc - 1)\n",
        "ks, thr_opt = ks_statistic(y_test.values, y_score)\n",
        "pr_auc = float(average_precision_score(y_test, y_score))\n",
        "\n",
        "y_pred_opt = (y_score >= thr_opt).astype(int)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_opt, average='binary', zero_division=0)\n",
        "acc = accuracy_score(y_test, y_pred_opt)\n",
        "cm = confusion_matrix(y_test, y_pred_opt)\n",
        "\n",
        "lift_top, capture_top = top_decile_lift(y_test.values, y_score)\n",
        "\n",
        "print({\n",
        "    'AUC': round(auc,4), 'Gini': round(gini,4), 'KS': round(ks,4), 'PR_AUC': round(pr_auc,4),\n",
        "    'thr_opt': round(thr_opt,4), 'precision': round(float(precision),4),\n",
        "    'recall': round(float(recall),4), 'f1': round(float(f1),4), 'acc': round(float(acc),4),\n",
        "    'lift_top_decile': round(lift_top,4), 'capture_top_decile': round(capture_top,4)\n",
        "})\n",
        "print('Matriz de confusão:\\n', cm)\n",
        "\n",
        "# ----------------- Exportação para Power BI -----------------\n",
        "# Score no conjunto de teste\n",
        "X_test_scored = X_test.reset_index(drop=True).copy()\n",
        "y_test_arr = y_test.reset_index(drop=True)\n",
        "X_test_scored['y_true'] = y_test_arr\n",
        "X_test_scored['y_score'] = y_score\n",
        "X_test_scored['y_pred_opt'] = (y_score >= thr_opt).astype(int)\n",
        "\n",
        "# Cria ID sintético se não houver\n",
        "if not any(c in X_test_scored.columns for c in ['id','member_id','loan_id']):\n",
        "    X_test_scored.insert(0, 'row_id', np.arange(len(X_test_scored)))\n",
        "\n",
        "X_test_scored.to_csv(DATA_PATH/'loan_scored.csv', index=False)\n",
        "print('Arquivo de dados salvo em', DATA_PATH/'loan_scored.csv')\n",
        "\n",
        "# Qualidade (pós-preprocess) — para painel de governança\n",
        "dq = build_data_dictionary(df)\n",
        "dq.to_csv(DATA_PATH/'data_quality_summary.csv', index=False)\n",
        "print('Qualidade salva em', DATA_PATH/'data_quality_summary.csv')\n",
        "\n",
        "# Métricas globais — para painel de performance\n",
        "run_ts = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%SZ')\n",
        "metrics = pd.DataFrame([{\n",
        "    'run_timestamp_utc': run_ts,\n",
        "    'model': best_model_name,\n",
        "    'train_rows': int(len(X_train)),\n",
        "    'test_rows': int(len(X_test)),\n",
        "    'features_num': int(len(num_cols)),\n",
        "    'features_cat': int(len(cat_cols)),\n",
        "    'default_rate_test': float(y_test.mean()),\n",
        "    'auc': float(auc),\n",
        "    'gini': float(gini),\n",
        "    'ks': float(ks),\n",
        "    'pr_auc': float(pr_auc),\n",
        "    'thr_opt': float(thr_opt),\n",
        "    'precision_opt': float(precision),\n",
        "    'recall_opt': float(recall),\n",
        "    'f1_opt': float(f1),\n",
        "    'accuracy_opt': float(acc),\n",
        "    'lift_top_decile': float(lift_top),\n",
        "    'capture_top_decile': float(capture_top)\n",
        "}])\n",
        "metrics.to_csv(DATA_PATH/'model_metrics.csv', index=False)\n",
        "print('Métricas salvas em', DATA_PATH/'model_metrics.csv')\n",
        "\n",
        "# Tabela única de KPIs — para a página principal do dashboard\n",
        "dashboard_table = metrics.copy()\n",
        "mean_missing_pct = dq['pct_missing'].mean() if not dq.empty else np.nan\n",
        "cols_high_missing = int((dq['pct_missing'] > 20).sum()) if not dq.empty else 0\n",
        "\n",
        "dashboard_table['avg_missing_pct'] = float(mean_missing_pct) if not np.isnan(mean_missing_pct) else np.nan\n",
        "dashboard_table['cols_missing_gt20pct'] = int(cols_high_missing)\n",
        "dashboard_table['dataset_rows_after_preprocess'] = int(len(df))\n",
        "dashboard_table['dataset_cols_after_preprocess'] = int(df.shape[1])\n",
        "dashboard_table.to_csv(DATA_PATH/'dashboard_table.csv', index=False)\n",
        "print('Tabela final para dashboard salva em', DATA_PATH/'dashboard_table.csv')\n",
        "\n",
        "print('\\n✅ Pipeline concluído.')\n",
        "print('Arquivos para o Power BI:')\n",
        "for f in ['loan_scored.csv','model_metrics.csv','data_quality_summary.csv','dashboard_table.csv']:\n",
        "    print(' -', DATA_PATH/f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVWFUXysWwZn",
        "outputId": "e67534f2-a10a-41f3-ab44-2c6d2c1a6874"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libs carregadas. pandas = 2.2.2\n",
            ">> Modo UPLOAD: usando arquivo LoanStats.csv existente\n",
            "Arquivo copiado para data/LoanStats.csv\n",
            "Arquivo detectado: data/LoanStats.csv\n",
            "Shape bruto: (42535, 144)\n",
            "Dicionário (raw) salvo em data/data_dictionary.csv\n",
            "Pulando geração do perfil para rodar mais leve.\n",
            "Shape após preprocessamento: (42535, 23)\n",
            "Numéricas: 15 | Categóricas: 7\n",
            "AUC Logistic Regression: 0.7025\n",
            "AUC XGBoost: 0.6978\n",
            "Melhor modelo: LogisticRegression | AUC: 0.7025\n",
            "{'AUC': 0.7025, 'Gini': 0.4049, 'KS': 0.3022, 'PR_AUC': 0.2963, 'thr_opt': 0.1456, 'precision': 0.2415, 'recall': 0.6858, 'f1': 0.3572, 'acc': 0.6269, 'lift_top_decile': 2.2543, 'capture_top_decile': 0.2255}\n",
            "Matriz de confusão:\n",
            " [[4451 2770]\n",
            " [ 404  882]]\n",
            "Arquivo de dados salvo em data/loan_scored.csv\n",
            "Qualidade salva em data/data_quality_summary.csv\n",
            "Métricas salvas em data/model_metrics.csv\n",
            "Tabela final para dashboard salva em data/dashboard_table.csv\n",
            "\n",
            "✅ Pipeline concluído.\n",
            "Arquivos para o Power BI:\n",
            " - data/loan_scored.csv\n",
            " - data/model_metrics.csv\n",
            " - data/data_quality_summary.csv\n",
            " - data/dashboard_table.csv\n"
          ]
        }
      ]
    }
  ]
}